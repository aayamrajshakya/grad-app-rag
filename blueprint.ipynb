{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7684a92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5307e13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")   # suppress all warnings for neatness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f5f785",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb3dcb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 359\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "with open(\"raw_data/demo.md\") as f:\n",
    "    file_content = f.read()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=170, chunk_overlap=20  # overlap to maintain context betn chunks\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text(file_content)  # list of split chunks\n",
    "print(f\"Total chunks: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf802dc",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9aeb5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "hf_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"all-mpnet-base-v2\", # is better than all-MiniLM-L6-v2\n",
    "    model_kwargs={\"device\": \"cpu\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": False},\n",
    "    multi_process=False,  # run encode() on multiple GPUs if True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abd37e0",
   "metadata": {},
   "source": [
    "### Vector storing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9159542a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "import os\n",
    "\n",
    "db_path = \"faiss_index\"\n",
    "if not os.path.isdir(db_path):\n",
    "    # from_texts method takes a list of raw texts and embeds them with the provided embedding model\n",
    "    faiss = FAISS.from_texts(texts=chunks, embedding=hf_model)\n",
    "    # saving the embeddings locally\n",
    "    faiss.save_local(folder_path=db_path)\n",
    "else:\n",
    "    print(\"Database already created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6f83340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vector store\n",
    "db = FAISS.load_local(folder_path=\"faiss_index\", embeddings=hf_model,\n",
    "                      allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd24b4b9",
   "metadata": {},
   "source": [
    "### Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48ad44f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_query = \"What is the deadline for spring 2026 semester?\"\n",
    "## FAISS uses the hf_model to convert query into embedding and then searches that embedding against the stored retriever = db\n",
    "## the 2nd arg specifies the top Kth nearest neighbors to retrieve\n",
    "# res = db.similarity_search(query=user_query, k=2)\n",
    "# [r.page_content for r in res]\n",
    "\n",
    "#****************************************************************#\n",
    "# Reference: https://shorturl.at/qnU8i (LangChain docs)\n",
    "\n",
    "## Retrieve more documents with higher diversity\n",
    "## Useful if your dataset has many similar documents\n",
    "# retriever = db.as_retriever(\n",
    "#     search_type=\"mmr\",\n",
    "#     search_kwargs={'k': 6, 'lambda_mult': 0.25}\n",
    "# )\n",
    "\n",
    "## Fetch more documents for the MMR algorithm to consider\n",
    "## But only return the top 5\n",
    "# retriever = db.as_retriever(\n",
    "#     search_type=\"mmr\",\n",
    "#     search_kwargs={'k': 5, 'fetch_k': 50}\n",
    "# )\n",
    "\n",
    "## Only retrieve documents that have a relevance score\n",
    "## Above a certain threshold\n",
    "# retriever = db.as_retriever(\n",
    "#     search_type=\"similarity_score_threshold\",\n",
    "#     search_kwargs={'score_threshold': 0.8}\n",
    "# )\n",
    "\n",
    "# Only get the single most similar document from the dataset\n",
    "retriever = db.as_retriever(search_kwargs={'k': 1})\n",
    "\n",
    "#****************************************************************#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee781c9a",
   "metadata": {},
   "source": [
    "### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00ce3e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://python.langchain.com/docs/integrations/chat/huggingface/\n",
    "\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=512,\n",
    "    do_sample=False,\n",
    "    repetition_penalty=1.03,\n",
    "    provider=\"auto\",  # let Hugging Face choose the best provider for you\n",
    ")\n",
    "\n",
    "chat_model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca29050c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context. If 100% out of context, immediately say you don't know.\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd360d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://python.langchain.com/docs/versions/migrating_chains/retrieval_qa/#lcel\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "qa_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | chat_model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# user_query = input(\"Enter your question: \")\n",
    "# qa_chain.invoke(input=user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "394aefc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Based on the provided context, the deadline for international graduate students applying to US campuses for the Fall 2026 semester at Northeastern University is August 30. However, please note that deadlines can change, so it's always a good idea to check the most recent information directly from the university's official website.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_1 = \"When is the deadline for international graduate students at Northeastern University?\"\n",
    "qa_chain.invoke(input=question_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "351471da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know. The provided context does not include information about deadlines for international graduate students at Harvard University. It only lists deadlines for domestic students at unspecified institutions for the Spring 2026 and Fall 2026 semesters.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_2 = \"When is the deadline for international graduate students at Harvard University?\"\n",
    "qa_chain.invoke(input=question_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
